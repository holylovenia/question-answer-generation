{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agus/anaconda3/envs/basic/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/agus/anaconda3/envs/basic/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from stanford_postagger.stanford_wrapper import StanfordPOSTagger as StanfordPOSTaggerWrapper\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import scipy\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conlltxt2dataset(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    del lines[0]\n",
    "    del lines[0]\n",
    "    \n",
    "    dataset = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        splitter = line.strip().split(' ')\n",
    "        if splitter[0] == '':\n",
    "            continue\n",
    "        elif (splitter[0] == '-DOCSTART-'):\n",
    "            dataset.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            token = splitter[0]\n",
    "            tag = splitter[3]\n",
    "            sentence.append((token, tag))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = convert_conlltxt2dataset('datasets/conll2003/train.txt')\n",
    "test_dataset = convert_conlltxt2dataset('datasets/conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'B-ORG'), ('rejects', 'O'), ('German', 'B-MISC')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_postag2dataset(dataset):\n",
    "    postagger = StanfordPOSTaggerWrapper()\n",
    "    dataset_with_postag = []\n",
    "    for sent in dataset:\n",
    "        postagged_sent = []\n",
    "        for index, (token, tag) in enumerate(sent):\n",
    "            postagged_token = postagger.tag(token)\n",
    "            postagged_sent.append((token, postagged_token[0][1], tag))\n",
    "        dataset_with_postag.append(postagged_sent)\n",
    "        \n",
    "    return dataset_with_postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "postagged_train_dataset = add_postag2dataset(train_dataset)\n",
    "postagged_test_dataset = add_postag2dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    # Ortographic Feature, Word, POSTag & N-Gram\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[:3]': word[:3],\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2]\n",
    "    }\n",
    "    \n",
    "    # Position\n",
    "    features.update({\n",
    "        'pos_front': i,\n",
    "        'pos_end': len(sent) - i\n",
    "    })\n",
    "    \n",
    "    # Bag Of Words\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2postag(sent):\n",
    "    return [postag for token, postag, label in sent]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'NNP', 'B-ORG'), ('rejects', 'VBZ', 'O'), ('German', 'JJ', 'B-MISC')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postagged_train_dataset[0][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(sent) for sent in postagged_train_dataset]\n",
    "y_train = [sent2labels(sent) for sent in postagged_train_dataset]\n",
    "\n",
    "X_test = [sent2features(sent) for sent in postagged_test_dataset]\n",
    "y_test = [sent2labels(sent) for sent in postagged_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_ = 0.001262621084804322\n",
    "c2_ = 0.07748342053200617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.7 s, sys: 776 ms, total: 31.5 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=c1_,\n",
    "    c2=c2_,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = crf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8276477008704834"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'finalized_model.sav'\n",
    "pickle.dump(crf, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict New String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = [['Beyoncé',\n",
    "  'Giselle',\n",
    "  'Knowles-Carter',\n",
    "  'bee-YON-say',\n",
    "  'born',\n",
    "  'September',\n",
    "  '1981',\n",
    "  'is',\n",
    "  'an',\n",
    "  'American',\n",
    "  'singer',\n",
    "  'songwriter',\n",
    "  'record',\n",
    "  'producer',\n",
    "  'and',\n",
    "  'actress'],\n",
    " ['Born',\n",
    "  'and',\n",
    "  'raised',\n",
    "  'in',\n",
    "  'Houston',\n",
    "  'Texas',\n",
    "  'she',\n",
    "  'performed',\n",
    "  'in',\n",
    "  'various',\n",
    "  'singing',\n",
    "  'and',\n",
    "  'dancing',\n",
    "  'competitions',\n",
    "  'as',\n",
    "  'a',\n",
    "  'child',\n",
    "  'and',\n",
    "  'rose',\n",
    "  'to',\n",
    "  'fame',\n",
    "  'in',\n",
    "  'the',\n",
    "  'late',\n",
    "  '1990s',\n",
    "  'as',\n",
    "  'lead',\n",
    "  'singer',\n",
    "  'of',\n",
    "  'R&B',\n",
    "  'girl-group',\n",
    "  'Destiny',\n",
    "  'Child'],\n",
    " ['Managed',\n",
    "  'by',\n",
    "  'her',\n",
    "  'father',\n",
    "  'Mathew',\n",
    "  'Knowles',\n",
    "  'the',\n",
    "  'group',\n",
    "  'became',\n",
    "  'one',\n",
    "  'of',\n",
    "  'the',\n",
    "  'world',\n",
    "  'best-selling',\n",
    "  'girl',\n",
    "  'groups',\n",
    "  'of',\n",
    "  'all',\n",
    "  'time'],\n",
    " ['Their',\n",
    "  'hiatus',\n",
    "  'saw',\n",
    "  'the',\n",
    "  'release',\n",
    "  'of',\n",
    "  'Beyoncé',\n",
    "  'debut',\n",
    "  'album',\n",
    "  'Dangerously',\n",
    "  'in',\n",
    "  'Love',\n",
    "  '2003',\n",
    "  'which',\n",
    "  'established',\n",
    "  'her',\n",
    "  'as',\n",
    "  'a',\n",
    "  'solo',\n",
    "  'artist',\n",
    "  'worldwide',\n",
    "  'earned',\n",
    "  'five',\n",
    "  'Grammy',\n",
    "  'Awards',\n",
    "  'and',\n",
    "  'featured',\n",
    "  'the',\n",
    "  'Billboard',\n",
    "  'Hot',\n",
    "  '100',\n",
    "  'number-one',\n",
    "  'singles',\n",
    "  'Crazy',\n",
    "  'in',\n",
    "  'Love',\n",
    "  'and',\n",
    "  'Baby',\n",
    "  'Boy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_other_label2dataset(dataset):\n",
    "    other_label_dataset = []\n",
    "    for sent in dataset:\n",
    "        sent_list = []\n",
    "        for token in sent:\n",
    "            sent_list.append((token, 'O'))\n",
    "        other_label_dataset.append(sent_list)\n",
    "    return other_label_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beyoncé', 'O'),\n",
       " ('Giselle', 'O'),\n",
       " ('Knowles-Carter', 'O'),\n",
       " ('bee-YON-say', 'O'),\n",
       " ('born', 'O'),\n",
       " ('September', 'O'),\n",
       " ('1981', 'O'),\n",
       " ('is', 'O'),\n",
       " ('an', 'O'),\n",
       " ('American', 'O'),\n",
       " ('singer', 'O'),\n",
       " ('songwriter', 'O'),\n",
       " ('record', 'O'),\n",
       " ('producer', 'O'),\n",
       " ('and', 'O'),\n",
       " ('actress', 'O')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_label_dataset = add_other_label2dataset(test_text_list)\n",
    "other_label_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beyoncé', 'NNP', 'O'),\n",
       " ('Giselle', 'NNP', 'O'),\n",
       " ('Knowles-Carter', 'NNP', 'O'),\n",
       " ('bee-YON-say', 'NN', 'O'),\n",
       " ('born', 'VBN', 'O'),\n",
       " ('September', 'NNP', 'O'),\n",
       " ('1981', 'CD', 'O'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('an', 'DT', 'O'),\n",
       " ('American', 'NNP', 'O'),\n",
       " ('singer', 'NN', 'O'),\n",
       " ('songwriter', 'NN', 'O'),\n",
       " ('record', 'NN', 'O'),\n",
       " ('producer', 'NN', 'O'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('actress', 'NN', 'O')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postagged_test = add_postag2dataset(other_label_dataset)\n",
    "postagged_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [sent2features(sent) for sent in postagged_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-PER',\n",
       "  'I-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'I-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'I-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'I-ORG']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'Doe',\n",
       " 'is',\n",
       " 'the',\n",
       " 'most',\n",
       " 'handsome',\n",
       " 'person',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"John Doe is the most handsome person in the world\"\n",
    "sentence = text.split()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_label_dataset = add_other_label2dataset([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "postagged_test = add_postag2dataset(other_label_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = [sent2features(sent) for sent in postagged_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with New Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.NER import NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model Success\n"
     ]
    }
   ],
   "source": [
    "ner = NER('finalized_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-PER',\n",
       "  'I-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'I-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'I-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'I-ORG']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict_class_text_list(test_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict_class_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'B-ORG': 0.0007559024654277962,\n",
       "   'O': 1.9755432037347083e-05,\n",
       "   'B-MISC': 5.523440012265124e-05,\n",
       "   'B-PER': 0.9962545385251304,\n",
       "   'I-PER': 0.0027229147485909066,\n",
       "   'B-LOC': 0.00011879024625599744,\n",
       "   'I-ORG': 5.3514887609208765e-05,\n",
       "   'I-MISC': 1.4866292352674246e-05,\n",
       "   'I-LOC': 4.4830024720018395e-06},\n",
       "  {'B-ORG': 3.699566917079212e-05,\n",
       "   'O': 7.134945874731716e-05,\n",
       "   'B-MISC': 2.3603605069636875e-05,\n",
       "   'B-PER': 0.00022768884042872998,\n",
       "   'I-PER': 0.9987219757091576,\n",
       "   'B-LOC': 1.994387821576716e-05,\n",
       "   'I-ORG': 0.0007598134366588535,\n",
       "   'I-MISC': 5.285984822983397e-05,\n",
       "   'I-LOC': 8.576955432094797e-05},\n",
       "  {'B-ORG': 9.635405742680737e-08,\n",
       "   'O': 0.9999340236308972,\n",
       "   'B-MISC': 1.251807318723364e-06,\n",
       "   'B-PER': 1.708631580181513e-07,\n",
       "   'I-PER': 5.899954012330967e-05,\n",
       "   'B-LOC': 4.391770904934039e-08,\n",
       "   'I-ORG': 9.026373087700323e-07,\n",
       "   'I-MISC': 3.735471748194755e-06,\n",
       "   'I-LOC': 7.757776787617347e-07},\n",
       "  {'B-ORG': 3.867632428295636e-06,\n",
       "   'O': 0.999990295335272,\n",
       "   'B-MISC': 3.3141716030355036e-07,\n",
       "   'B-PER': 5.544536562453889e-07,\n",
       "   'I-PER': 3.2949496645442794e-06,\n",
       "   'B-LOC': 6.978898256232373e-07,\n",
       "   'I-ORG': 2.280181254471296e-07,\n",
       "   'I-MISC': 5.324133043067447e-07,\n",
       "   'I-LOC': 1.9789056275073478e-07},\n",
       "  {'B-ORG': 2.4081101099750417e-05,\n",
       "   'O': 0.9999280873607622,\n",
       "   'B-MISC': 3.966405644237178e-05,\n",
       "   'B-PER': 5.77982370318621e-07,\n",
       "   'I-PER': 6.266347148802133e-09,\n",
       "   'B-LOC': 7.4098000571235434e-06,\n",
       "   'I-ORG': 5.054840751438635e-09,\n",
       "   'I-MISC': 9.806672293167733e-08,\n",
       "   'I-LOC': 7.031135692080964e-08},\n",
       "  {'B-ORG': 3.378095139747766e-06,\n",
       "   'O': 0.9997768107811379,\n",
       "   'B-MISC': 0.00019856613887343089,\n",
       "   'B-PER': 4.4493465780391675e-07,\n",
       "   'I-PER': 6.54463588171067e-10,\n",
       "   'B-LOC': 2.0638360690949e-05,\n",
       "   'I-ORG': 2.2484617351735592e-08,\n",
       "   'I-MISC': 1.335819997136647e-07,\n",
       "   'I-LOC': 4.9684190122175526e-09},\n",
       "  {'B-ORG': 0.00017528169525994188,\n",
       "   'O': 0.9995129537800556,\n",
       "   'B-MISC': 6.978916773416951e-05,\n",
       "   'B-PER': 0.00016027196347451477,\n",
       "   'I-PER': 1.1543751856862957e-06,\n",
       "   'B-LOC': 7.708734085730666e-05,\n",
       "   'I-ORG': 6.072043746155382e-07,\n",
       "   'I-MISC': 1.0060642100146263e-06,\n",
       "   'I-LOC': 1.848408847650806e-06},\n",
       "  {'B-ORG': 1.0206251444529371e-07,\n",
       "   'O': 0.9999990279011848,\n",
       "   'B-MISC': 6.523220301795744e-08,\n",
       "   'B-PER': 1.4518388198264086e-07,\n",
       "   'I-PER': 6.964024693526967e-08,\n",
       "   'B-LOC': 3.2991543897310896e-08,\n",
       "   'I-ORG': 3.2136811342407695e-07,\n",
       "   'I-MISC': 1.453362509354959e-07,\n",
       "   'I-LOC': 9.028405977318339e-08},\n",
       "  {'B-ORG': 1.3063010110148329e-06,\n",
       "   'O': 0.999976147846229,\n",
       "   'B-MISC': 1.010083883365397e-05,\n",
       "   'B-PER': 5.481255688979917e-08,\n",
       "   'I-PER': 1.7147587501532399e-06,\n",
       "   'B-LOC': 7.591873845543463e-06,\n",
       "   'I-ORG': 1.1245141797757728e-06,\n",
       "   'I-MISC': 8.697115335118194e-07,\n",
       "   'I-LOC': 1.0893430597857155e-06},\n",
       "  {'B-ORG': 3.255707388695469e-05,\n",
       "   'O': 0.9997805846041325,\n",
       "   'B-MISC': 0.00016686635607180117,\n",
       "   'B-PER': 2.9078833496360694e-06,\n",
       "   'I-PER': 7.287964307389132e-08,\n",
       "   'B-LOC': 1.6359604678503273e-05,\n",
       "   'I-ORG': 6.40506543625869e-09,\n",
       "   'I-MISC': 3.327287940852339e-07,\n",
       "   'I-LOC': 3.12464377589709e-07}]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict_marginal_class_text(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic]",
   "language": "python",
   "name": "conda-env-basic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
