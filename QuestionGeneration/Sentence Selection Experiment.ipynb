{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy Lovenia / 13515113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset/SQuAD/train-v2.0.json') as f:\n",
    "    json_data = json.load(f)['data']\n",
    "\n",
    "    for i in range(len(json_data)):\n",
    "        json_data_i = json_data[i]['paragraphs']\n",
    "        \n",
    "        for j in range(1):\n",
    "            paragraph = json_data_i[j]['context']\n",
    "                \n",
    "            data.append(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = []\n",
    "with open('dataset/SQuAD/train-v2.0.json') as f:\n",
    "    json_data = json.load(f)['data']\n",
    "\n",
    "    for i in range(len(json_data)):\n",
    "        json_data_i = json_data[i]['paragraphs']\n",
    "        \n",
    "        for j in range(1):\n",
    "            paragraph = json_data_i[j]['context']\n",
    "            \n",
    "            # replace all dictionary phonetic with ''\n",
    "            paragraph = re.sub('\\/.*\\ˈ.*\\/', '', paragraph)\n",
    "            \n",
    "            # replace all japanese characters with ''\n",
    "            paragraph = re.sub('[\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbf]+', '', paragraph)\n",
    "            paragraph = re.sub(r'[^\\x00-\\x7f]',r'', paragraph) \n",
    "            \n",
    "            # replace dots in the center of words\n",
    "            words = paragraph.split(' ')\n",
    "            for i in range(len(words)):\n",
    "                if(words[i].find('.') != len(words[i]) - 1 and words[i].find('.') != -1):\n",
    "                    words[i] = words[i].replace('.', '')\n",
    "                if(words[i].find(',') != len(words[i]) - 1 and words[i].find(',') != -1):\n",
    "                    words[i] = words[i].replace(',', '')\n",
    "\n",
    "            paragraph = ' '.join(words)\n",
    "            \n",
    "            data_i_j = paragraph.split('.')\n",
    "            \n",
    "            paragraph = []\n",
    "            for k in range(len(data_i_j)):\n",
    "                tokenizer = RegexpTokenizer('[\\w\\/\\&\\-\\:]+', flags=re.UNICODE)\n",
    "    \n",
    "                token_list = tokenizer.tokenize(data_i_j[k])\n",
    "                token_list = [token.strip() for token in token_list if len(token.strip()) > 1 or token.lower() == 'a']\n",
    "            \n",
    "                if token_list != []:\n",
    "                    paragraph.append(token_list)\n",
    "                \n",
    "            preprocessed_data.append(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_selection.lsa import SteinbergerJezekLSA\n",
    "from sentence_selection.text_rank import TextRank\n",
    "from sentence_selection.multi_word_phrase_extraction import MultiWordPhraseExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_jaccard = TextRank(similarity='jaccard')\n",
    "tr_cosine = TextRank(similarity='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiWordPhraseExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwpe = MultiWordPhraseExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_tfidf = SteinbergerJezekLSA(matrix_technique='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_binary = SteinbergerJezekLSA(matrix_technique='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 2] [3, 1] [0, 1] [0, 3] [1, 2]\n",
      "1 [2, 0] [1, 0] [0, 1] [1, 0] [2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/holy/Projects/question-answer-generation/sentence_selection/text_rank.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [5, 4] [2, 6] [2, 4] [4, 1] [6, 0]\n",
      "3 [2, 1] [0, 3] [0, 1] [1, 2] [3, 1]\n",
      "4 [4, 1] [2, 0] [0, 2] [4, 2] [3, 0]\n",
      "5 [3, 0] [1, 2] [2, 0] [0, 2] [1, 3]\n",
      "6 passage only has 1 sentence\n",
      "7 [1, 2] [0, 1] [1, 0] [0, 1] [2, 1]\n",
      "8 [0, 1] [2, 1] [1, 2] [0, 1] [1, 0]\n",
      "9 passage only has 1 sentence\n",
      "10 [0, 1] [1, 0] [0, 1] [1, 0] [0, 1]\n",
      "11 [2, 0] [1, 0] [0, 1] [0, 1] [2, 0]\n",
      "12 [2, 1] [3, 1] [3, 0] [1, 3] [2, 3]\n",
      "13 passage only has 1 sentence\n",
      "14 [1, 0] [2, 0] [0, 2] [2, 1] [0, 1]\n",
      "15 [1, 2] [0, 2] [0, 2] [1, 2] [0, 1]\n",
      "16 [3, 0] [2, 0] [0, 1] [4, 3] [3, 1]\n",
      "17 [1, 0] [1, 0] [0, 1] [1, 0] [0, 1]\n",
      "18 [1, 0] [3, 1] [0, 1] [3, 1] [3, 2]\n",
      "19 [1, 2] [0, 2] [1, 2] [1, 0] [2, 0]\n",
      "20 [1, 2] [0, 2] [0, 2] [2, 0] [1, 2]\n",
      "21 [1, 2] [0, 2] [0, 1] [1, 2] [1, 2]\n",
      "22 [0, 2] [1, 2] [2, 0] [2, 1] [2, 0]\n",
      "23 passage only has 1 sentence\n",
      "24 [0, 1] [1, 0] [0, 1] [0, 1] [1, 0]\n",
      "25 [1, 2] [2, 3] [0, 1] [1, 3] [3, 1]\n",
      "26 [4, 3] [1, 4] [4, 0] [1, 2] [0, 3]\n",
      "27 [2, 1] [3, 0] [3, 0] [1, 0] [4, 0]\n",
      "28 [2, 1] [0, 1] [0, 1] [1, 0] [1, 0]\n",
      "29 [1, 2] [2, 0] [0, 1] [1, 0] [0, 2]\n",
      "30 [1, 0] [1, 0] [1, 0] [0, 1] [1, 0]\n",
      "31 [0, 1] [1, 0] [0, 1] [0, 1] [1, 0]\n",
      "32 [1, 0] [0, 1] [0, 1] [0, 1] [0, 1]\n",
      "33 passage only has 1 sentence\n",
      "34 [1, 0] [2, 0] [2, 0] [1, 2] [2, 1]\n",
      "35 [1, 0] [2, 0] [0, 1] [0, 2] [2, 1]\n",
      "36 passage only has 1 sentence\n",
      "37 [1, 0] [0, 1] [1, 0] [1, 0] [1, 0]\n",
      "38 [0, 3] [1, 2] [0, 1] [5, 0] [5, 2]\n",
      "39 [2, 1] [5, 4] [4, 0] [2, 1] [4, 5]\n",
      "40 passage only has 1 sentence\n",
      "41 [0, 2] [1, 2] [2, 0] [2, 1] [0, 2]\n",
      "42 [3, 0] [1, 4] [1, 0] [2, 4] [4, 0]\n",
      "43 passage only has 1 sentence\n",
      "44 [1, 3] [0, 1] [1, 0] [0, 2] [2, 0]\n",
      "45 [0, 1] [1, 0] [1, 0] [0, 1] [1, 0]\n",
      "46 [3, 5] [6, 4] [2, 0] [5, 2] [0, 6]\n",
      "47 [3, 2] [0, 2] [1, 2] [1, 2] [1, 2]\n",
      "48 [0, 1] [1, 0] [1, 0] [1, 0] [0, 1]\n",
      "49 [3, 1] [2, 3] [2, 0] [1, 3] [1, 3]\n",
      "50 [2, 7] [4, 8] [3, 4] [8, 7] [1, 4]\n",
      "51 [2, 1] [1, 0] [1, 2] [0, 1] [0, 1]\n",
      "52 [0, 1] [1, 0] [0, 1] [1, 0] [1, 0]\n",
      "53 [2, 3] [0, 1] [0, 1] [1, 0] [1, 3]\n",
      "54 [2, 0] [6, 4] [1, 4] [5, 3] [6, 4]\n",
      "55 passage only has 1 sentence\n",
      "56 [1, 3] [2, 3] [2, 0] [0, 3] [0, 4]\n",
      "57 [7, 0] [1, 5] [6, 0] [7, 4] [3, 4]\n",
      "58 [1, 0] [0, 1] [0, 1] [1, 0] [1, 0]\n",
      "59 [1, 0] [0, 1] [1, 0] [1, 0] [1, 0]\n",
      "60 [1, 0] [0, 1] [0, 1] [0, 1] [1, 0]\n",
      "61 [1, 0] [0, 1] [0, 1] [1, 0] [1, 0]\n",
      "62 passage only has 1 sentence\n",
      "63 [2, 0] [0, 1] [0, 1] [1, 0] [0, 2]\n",
      "64 [1, 0] [2, 1] [0, 1] [1, 0] [2, 1]\n",
      "65 [1, 0] [2, 1] [1, 0] [1, 2] [2, 1]\n",
      "66 [3, 5] [4, 0] [0, 1] [6, 7] [3, 1]\n",
      "67 [2, 0] [2, 1] [0, 1] [0, 3] [1, 0]\n",
      "68 passage only has 1 sentence\n",
      "69 [2, 1] [0, 1] [0, 1] [3, 1] [0, 1]\n",
      "70 [4, 2] [1, 3] [1, 0] [2, 0] [2, 0]\n",
      "71 [0, 1] [1, 0] [0, 1] [0, 1] [0, 1]\n",
      "72 [0, 2] [3, 1] [1, 2] [1, 0] [1, 0]\n",
      "73 [0, 2] [4, 3] [0, 2] [2, 0] [1, 2]\n",
      "74 [3, 1] [0, 2] [0, 1] [4, 3] [1, 0]\n",
      "75 [2, 1] [1, 0] [1, 0] [0, 1] [0, 2]\n",
      "76 [3, 0] [2, 0] [0, 2] [4, 3] [0, 5]\n",
      "77 [4, 3] [0, 2] [0, 1] [0, 4] [0, 4]\n",
      "78 [0, 1] [1, 0] [1, 0] [1, 0] [0, 1]\n",
      "79 [2, 1] [0, 1] [0, 1] [1, 0] [2, 0]\n",
      "80 [2, 1] [0, 3] [0, 1] [2, 5] [4, 2]\n",
      "81 [1, 0] [2, 0] [2, 0] [1, 0] [1, 2]\n",
      "82 [1, 0] [0, 1] [0, 1] [1, 0] [0, 1]\n",
      "83 [3, 0] [2, 0] [0, 1] [1, 2] [1, 3]\n",
      "84 [1, 4] [5, 1] [1, 0] [0, 4] [3, 2]\n",
      "85 [0, 2] [1, 2] [1, 0] [2, 1] [1, 0]\n",
      "86 [1, 0] [1, 0] [0, 1] [1, 0] [1, 0]\n",
      "87 passage only has 1 sentence\n",
      "88 [0, 1] [1, 0] [1, 0] [0, 1] [0, 1]\n",
      "89 [0, 3] [2, 1] [1, 0] [1, 3] [2, 0]\n",
      "90 [2, 0] [3, 0] [0, 1] [2, 1] [0, 3]\n",
      "91 [0, 1] [0, 2] [0, 2] [3, 0] [3, 1]\n",
      "92 [0, 2] [1, 0] [1, 0] [0, 1] [2, 1]\n",
      "93 [2, 0] [1, 0] [0, 1] [1, 2] [1, 0]\n",
      "94 [1, 0] [0, 2] [0, 1] [1, 0] [2, 1]\n",
      "95 [0, 1] [2, 3] [0, 1] [0, 2] [1, 2]\n",
      "96 [0, 1] [0, 1] [0, 1] [1, 0] [1, 0]\n",
      "97 [1, 0] [3, 2] [3, 0] [3, 1] [0, 1]\n",
      "98 [0, 1] [1, 2] [1, 2] [0, 2] [2, 0]\n",
      "99 [0, 2] [1, 2] [2, 0] [0, 1] [1, 0]\n",
      "100 [0, 1] [1, 0] [0, 1] [0, 1] [1, 0]\n",
      "101 [1, 2] [4, 2] [2, 3] [5, 3] [5, 3]\n",
      "102 [1, 2] [2, 0] [1, 0] [0, 1] [1, 0]\n",
      "103 [1, 3] [0, 2] [0, 2] [1, 0] [0, 2]\n",
      "104 [1, 0] [0, 1] [0, 1] [0, 1] [0, 1]\n",
      "105 [2, 1] [0, 4] [0, 1] [3, 1] [0, 2]\n",
      "106 [1, 0] [3, 2] [0, 1] [0, 2] [0, 3]\n",
      "107 [3, 5] [0, 1] [0, 2] [9, 5] [3, 6]\n",
      "108 [2, 3] [0, 1] [0, 1] [1, 3] [0, 1]\n",
      "109 [0, 1] [1, 0] [0, 1] [0, 1] [1, 0]\n",
      "110 [2, 1] [0, 3] [3, 0] [2, 0] [1, 3]\n",
      "111 [5, 7] [6, 8] [1, 6] [0, 5] [6, 2]\n",
      "112 [4, 1] [0, 3] [0, 1] [2, 0] [2, 0]\n",
      "113 [1, 0] [1, 0] [1, 0] [0, 1] [1, 0]\n",
      "114 [1, 2] [0, 3] [0, 1] [0, 1] [3, 1]\n",
      "115 [0, 1] [1, 2] [2, 0] [2, 1] [1, 0]\n",
      "116 [2, 0] [1, 3] [3, 0] [0, 5] [3, 4]\n",
      "117 [0, 1] [4, 3] [2, 3] [4, 1] [2, 6]\n",
      "118 [4, 0] [3, 1] [3, 0] [0, 5] [8, 5]\n",
      "119 [2, 1] [0, 1] [1, 0] [0, 2] [1, 2]\n",
      "120 [8, 5] [1, 2] [6, 5] [7, 8] [4, 3]\n",
      "121 [0, 3] [6, 2] [6, 1] [7, 3] [0, 1]\n",
      "122 [0, 2] [2, 1] [2, 1] [3, 1] [3, 1]\n",
      "123 [1, 0] [2, 1] [0, 1] [0, 2] [2, 1]\n",
      "124 [1, 3] [6, 4] [3, 5] [0, 3] [5, 6]\n",
      "125 [4, 0] [6, 2] [0, 1] [4, 3] [6, 3]\n",
      "126 [5, 1] [3, 4] [1, 3] [5, 2] [0, 2]\n",
      "127 [1, 3] [0, 2] [2, 0] [3, 0] [3, 1]\n",
      "128 [1, 4] [0, 4] [0, 1] [1, 3] [1, 3]\n",
      "129 [1, 0] [2, 1] [0, 2] [0, 2] [2, 0]\n",
      "130 [3, 9] [4, 8] [5, 0] [7, 2] [3, 8]\n",
      "131 [1, 0] [0, 1] [1, 0] [1, 0] [1, 0]\n",
      "132 [3, 0] [1, 2] [0, 1] [3, 2] [2, 3]\n",
      "133 [1, 3] [0, 1] [1, 0] [3, 2] [3, 1]\n",
      "134 [0, 1] [2, 3] [0, 1] [0, 2] [0, 2]\n",
      "135 [8, 5] [7, 9] [7, 0] [4, 8] [0, 6]\n",
      "136 [2, 1] [3, 1] [1, 3] [1, 2] [3, 1]\n",
      "137 [5, 8] [1, 6] [0, 8] [5, 3] [5, 7]\n",
      "138 [4, 3] [5, 0] [4, 0] [1, 3] [4, 2]\n",
      "139 [3, 1] [0, 1] [1, 2] [1, 2] [3, 1]\n",
      "140 [6, 4] [2, 7] [7, 0] [4, 6] [7, 5]\n",
      "141 [1, 0] [2, 3] [0, 1] [3, 1] [3, 1]\n",
      "142 [1, 2] [0, 3] [2, 0] [1, 2] [3, 0]\n",
      "143 [10, 1] [3, 7] [3, 0] [0, 9] [2, 6]\n",
      "144 [1, 2] [4, 0] [1, 3] [1, 2] [2, 3]\n",
      "145 [2, 1] [0, 4] [0, 1] [2, 3] [3, 2]\n",
      "146 [3, 1] [0, 1] [0, 1] [3, 0] [1, 3]\n",
      "147 [3, 4] [0, 1] [0, 2] [0, 2] [0, 1]\n",
      "148 [1, 3] [0, 2] [3, 0] [2, 3] [0, 3]\n",
      "149 [2, 3] [3, 0] [3, 0] [1, 3] [2, 3]\n",
      "150 [0, 2] [1, 2] [1, 0] [2, 1] [2, 0]\n",
      "151 [4, 5] [1, 3] [1, 6] [4, 3] [3, 6]\n",
      "152 [5, 1] [3, 4] [4, 0] [5, 4] [2, 4]\n",
      "153 [0, 1] [2, 0] [0, 2] [0, 2] [2, 0]\n",
      "154 [2, 0] [1, 5] [1, 2] [3, 2] [3, 5]\n",
      "155 [1, 2] [2, 0] [2, 0] [1, 0] [2, 0]\n",
      "156 [3, 2] [4, 1] [2, 3] [3, 2] [4, 1]\n",
      "157 [3, 2] [4, 1] [0, 2] [0, 3] [0, 3]\n",
      "158 [3, 1] [0, 3] [0, 1] [3, 0] [0, 3]\n",
      "159 [0, 3] [6, 1] [2, 4] [5, 1] [3, 5]\n",
      "160 [7, 1] [6, 5] [1, 4] [7, 6] [6, 3]\n",
      "161 [4, 1] [2, 0] [2, 0] [2, 4] [0, 2]\n",
      "162 [4, 1] [0, 3] [0, 2] [2, 0] [2, 3]\n",
      "163 [2, 0] [1, 4] [0, 1] [2, 1] [0, 1]\n",
      "164 [2, 5] [0, 6] [6, 0] [2, 4] [1, 3]\n",
      "165 [3, 4] [7, 1] [1, 0] [5, 3] [6, 3]\n",
      "166 [0, 3] [1, 4] [4, 0] [0, 4] [4, 3]\n",
      "167 [4, 2] [0, 3] [0, 2] [2, 3] [1, 4]\n",
      "168 [4, 6] [0, 1] [3, 2] [1, 5] [1, 4]\n",
      "169 [3, 1] [2, 0] [0, 2] [0, 3] [1, 2]\n",
      "170 [3, 0] [1, 4] [1, 2] [0, 4] [1, 0]\n",
      "171 [4, 2] [1, 3] [4, 3] [1, 4] [2, 4]\n",
      "172 [3, 0] [2, 4] [4, 0] [1, 2] [2, 0]\n",
      "173 [8, 9] [6, 3] [6, 7] [5, 2] [8, 1]\n",
      "174 [2, 5] [0, 1] [1, 0] [2, 6] [1, 4]\n",
      "175 [5, 4] [2, 3] [4, 3] [4, 6] [2, 3]\n",
      "176 [3, 4] [1, 4] [1, 0] [2, 3] [2, 1]\n",
      "177 [3, 0] [1, 2] [3, 0] [3, 2] [0, 1]\n",
      "178 [0, 1] [1, 0] [0, 1] [1, 0] [0, 1]\n",
      "179 [2, 3] [0, 2] [2, 0] [0, 1] [0, 2]\n",
      "180 [4, 0] [0, 2] [2, 0] [1, 4] [3, 4]\n",
      "181 [6, 2] [3, 5] [6, 0] [6, 3] [3, 6]\n",
      "182 [2, 4] [1, 0] [1, 0] [2, 0] [2, 1]\n",
      "183 [4, 3] [2, 1] [0, 1] [4, 2] [0, 1]\n",
      "184 [1, 2] [0, 3] [0, 1] [1, 0] [1, 0]\n",
      "185 [2, 1] [0, 1] [0, 1] [0, 2] [0, 2]\n",
      "186 [0, 5] [4, 1] [1, 0] [0, 1] [1, 2]\n",
      "187 [3, 1] [0, 2] [0, 1] [3, 0] [2, 0]\n",
      "188 [3, 2] [1, 0] [0, 1] [0, 1] [1, 2]\n",
      "189 [2, 0] [1, 0] [0, 1] [1, 0] [0, 1]\n",
      "190 [1, 2] [0, 3] [0, 3] [0, 1] [0, 1]\n",
      "191 [2, 0] [1, 0] [1, 2] [0, 1] [1, 2]\n",
      "192 [1, 4] [0, 2] [2, 3] [4, 2] [1, 2]\n",
      "193 [3, 1] [2, 0] [0, 2] [0, 3] [0, 3]\n",
      "194 [5, 1] [0, 7] [0, 1] [1, 2] [4, 6]\n",
      "195 passage only has 1 sentence\n",
      "196 [4, 0] [3, 0] [4, 3] [2, 4] [1, 0]\n",
      "197 [2, 0] [1, 0] [0, 1] [1, 0] [1, 0]\n",
      "198 [3, 2] [0, 1] [0, 1] [2, 4] [4, 1]\n",
      "199 [0, 1] [1, 0] [0, 1] [0, 1] [1, 0]\n",
      "200 [5, 4] [1, 0] [0, 1] [3, 5] [3, 5]\n",
      "201 [3, 10] [4, 1] [1, 0] [2, 3] [10, 2]\n",
      "202 [0, 3] [2, 7] [7, 0] [0, 4] [2, 4]\n",
      "203 [8, 2] [5, 7] [1, 7] [3, 2] [4, 2]\n",
      "204 [2, 4] [6, 1] [3, 8] [2, 0] [5, 0]\n",
      "205 [1, 2] [0, 2] [0, 2] [0, 1] [2, 1]\n",
      "206 [4, 1] [3, 5] [0, 5] [1, 0] [1, 5]\n",
      "207 [1, 4] [3, 2] [2, 0] [4, 2] [0, 4]\n",
      "208 [3, 4] [1, 0] [0, 2] [2, 1] [4, 2]\n",
      "209 [2, 1] [1, 0] [2, 0] [2, 1] [1, 2]\n",
      "210 [4, 3] [0, 1] [0, 1] [3, 0] [1, 2]\n",
      "211 [3, 2] [4, 1] [4, 0] [2, 3] [2, 1]\n",
      "212 [0, 2] [1, 2] [1, 0] [2, 0] [2, 0]\n",
      "213 [1, 3] [6, 0] [0, 5] [1, 2] [4, 5]\n",
      "214 [3, 1] [2, 5] [4, 5] [1, 4] [0, 3]\n",
      "215 [1, 0] [3, 2] [4, 0] [0, 4] [2, 0]\n",
      "216 [0, 3] [0, 2] [0, 1] [1, 2] [1, 0]\n",
      "217 [0, 3] [4, 2] [0, 1] [1, 2] [4, 2]\n",
      "218 [2, 5] [3, 1] [3, 2] [0, 5] [2, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 [2, 0] [1, 3] [0, 1] [2, 3] [0, 2]\n",
      "220 [3, 1] [2, 5] [5, 0] [3, 1] [4, 2]\n",
      "221 [0, 1] [2, 0] [0, 1] [2, 0] [1, 0]\n",
      "222 [8, 7] [1, 2] [2, 9] [8, 1] [8, 6]\n",
      "223 [1, 3] [4, 2] [4, 0] [0, 1] [2, 3]\n",
      "224 [4, 2] [0, 3] [0, 1] [1, 3] [1, 3]\n",
      "225 [1, 0] [2, 3] [1, 0] [0, 2] [2, 0]\n",
      "226 [2, 4] [0, 1] [0, 3] [4, 3] [1, 4]\n",
      "227 [4, 2] [0, 1] [0, 2] [4, 0] [4, 1]\n",
      "228 [1, 2] [0, 3] [0, 1] [4, 3] [4, 2]\n",
      "229 [0, 2] [1, 3] [0, 1] [0, 1] [3, 0]\n",
      "230 [3, 5] [4, 1] [6, 0] [8, 0] [2, 1]\n",
      "231 [3, 4] [2, 6] [2, 0] [2, 5] [0, 1]\n",
      "232 [4, 1] [2, 0] [1, 0] [1, 7] [5, 7]\n",
      "233 [2, 4] [4, 0] [2, 3] [3, 1] [3, 4]\n",
      "234 [3, 1] [4, 2] [4, 0] [1, 3] [1, 3]\n",
      "235 [2, 0] [3, 1] [3, 0] [0, 3] [2, 1]\n",
      "236 [3, 1] [1, 0] [1, 0] [3, 1] [1, 3]\n",
      "237 [1, 3] [2, 6] [6, 0] [4, 1] [3, 6]\n",
      "238 [1, 3] [5, 4] [0, 5] [1, 5] [2, 3]\n",
      "239 [2, 0] [4, 1] [0, 1] [0, 3] [0, 3]\n",
      "240 [6, 1] [4, 2] [4, 0] [0, 1] [6, 0]\n",
      "241 [2, 6] [1, 0] [1, 5] [3, 4] [2, 5]\n",
      "242 [6, 3] [1, 0] [4, 0] [3, 1] [4, 0]\n",
      "243 [2, 0] [1, 2] [1, 0] [1, 2] [1, 2]\n",
      "244 [1, 0] [0, 2] [0, 1] [1, 0] [1, 2]\n",
      "245 [2, 1] [0, 3] [0, 1] [2, 3] [3, 1]\n",
      "246 [3, 2] [0, 1] [0, 1] [3, 1] [0, 3]\n",
      "247 [2, 5] [4, 3] [4, 0] [1, 5] [6, 5]\n",
      "248 [2, 3] [0, 3] [0, 1] [1, 3] [1, 3]\n",
      "249 [1, 2] [3, 0] [0, 3] [2, 3] [0, 3]\n",
      "250 [3, 2] [1, 0] [0, 1] [3, 1] [1, 3]\n",
      "251 [1, 0] [2, 0] [2, 0] [1, 2] [0, 1]\n",
      "252 [0, 4] [5, 3] [3, 4] [0, 3] [2, 4]\n",
      "253 [2, 3] [0, 1] [1, 0] [0, 2] [3, 1]\n",
      "254 [1, 2] [3, 0] [1, 2] [1, 0] [0, 1]\n",
      "255 [2, 0] [1, 0] [1, 0] [0, 1] [1, 2]\n",
      "256 [6, 5] [4, 3] [0, 2] [5, 2] [6, 5]\n",
      "257 [3, 1] [0, 3] [0, 3] [2, 3] [1, 0]\n",
      "258 [4, 0] [3, 2] [3, 0] [1, 3] [2, 3]\n",
      "259 [4, 2] [0, 3] [0, 1] [3, 4] [5, 2]\n",
      "260 [1, 0] [3, 2] [1, 0] [2, 1] [3, 1]\n",
      "261 [1, 3] [2, 0] [2, 0] [2, 0] [0, 2]\n",
      "262 [1, 0] [2, 0] [2, 0] [1, 2] [1, 0]\n",
      "263 [1, 2] [4, 0] [4, 0] [0, 4] [3, 2]\n",
      "264 [3, 1] [2, 0] [1, 0] [3, 2] [2, 0]\n",
      "265 [2, 3] [1, 0] [0, 1] [0, 3] [2, 1]\n",
      "266 [1, 4] [3, 0] [0, 1] [3, 4] [2, 1]\n",
      "267 [1, 0] [0, 1] [1, 0] [0, 1] [1, 0]\n",
      "268 [2, 3] [1, 3] [1, 2] [2, 1] [1, 2]\n",
      "269 [0, 2] [3, 1] [2, 3] [2, 1] [0, 2]\n",
      "270 [1, 0] [1, 3] [3, 0] [2, 1] [1, 3]\n",
      "271 [1, 0] [0, 2] [0, 1] [2, 0] [2, 0]\n",
      "272 [2, 1] [0, 2] [1, 0] [2, 0] [0, 2]\n",
      "273 [3, 0] [2, 1] [2, 0] [0, 1] [2, 3]\n",
      "274 [1, 3] [2, 0] [1, 0] [1, 4] [1, 2]\n",
      "275 [2, 4] [5, 3] [5, 0] [2, 5] [6, 1]\n",
      "276 passage only has 1 sentence\n",
      "277 [7, 1] [3, 2] [0, 4] [1, 6] [5, 6]\n",
      "278 [4, 5] [2, 0] [0, 1] [3, 5] [0, 2]\n",
      "279 [0, 3] [1, 3] [0, 1] [2, 1] [2, 0]\n",
      "280 [3, 1] [0, 1] [2, 3] [3, 0] [0, 2]\n",
      "281 [2, 4] [1, 3] [1, 3] [2, 4] [5, 1]\n",
      "282 [5, 1] [0, 4] [0, 4] [2, 5] [0, 4]\n",
      "283 [5, 6] [3, 0] [0, 1] [1, 5] [1, 6]\n",
      "284 passage only has 1 sentence\n",
      "285 [1, 0] [2, 0] [0, 1] [2, 0] [0, 2]\n",
      "286 [2, 0] [0, 2] [0, 1] [3, 2] [3, 0]\n",
      "287 [1, 0] [2, 0] [1, 2] [0, 2] [2, 0]\n",
      "288 [4, 3] [2, 0] [0, 1] [0, 2] [3, 0]\n",
      "289 [2, 0] [8, 3] [6, 1] [8, 2] [0, 5]\n",
      "290 [0, 1] [1, 0] [0, 1] [1, 0] [1, 0]\n",
      "291 [5, 6] [3, 2] [3, 4] [0, 1] [5, 4]\n",
      "292 [0, 3] [1, 4] [2, 3] [0, 3] [2, 3]\n",
      "293 [1, 4] [2, 3] [1, 2] [3, 2] [1, 0]\n",
      "294 [0, 1] [2, 0] [0, 1] [0, 2] [2, 0]\n",
      "295 [2, 1] [4, 5] [2, 1] [2, 3] [0, 2]\n",
      "296 [1, 3] [0, 2] [0, 1] [2, 3] [0, 1]\n",
      "297 [9, 3] [4, 5] [1, 0] [9, 0] [8, 4]\n",
      "298 [0, 2] [0, 1] [0, 1] [1, 0] [1, 0]\n",
      "299 [1, 0] [2, 3] [0, 1] [1, 4] [5, 4]\n",
      "300 [0, 1] [3, 2] [1, 0] [0, 2] [3, 1]\n",
      "301 [0, 2] [1, 3] [0, 1] [2, 3] [0, 4]\n",
      "302 [1, 3] [0, 2] [0, 2] [1, 0] [0, 3]\n",
      "303 [0, 1] [3, 1] [0, 3] [3, 0] [0, 1]\n",
      "304 [7, 3] [1, 5] [1, 0] [4, 2] [2, 0]\n",
      "305 [1, 2] [0, 3] [0, 1] [3, 1] [3, 2]\n",
      "306 [1, 2] [1, 0] [0, 3] [3, 2] [3, 2]\n",
      "307 [3, 1] [0, 3] [0, 1] [1, 0] [1, 0]\n",
      "308 [0, 1] [1, 0] [1, 0] [0, 1] [1, 0]\n",
      "309 [1, 3] [0, 4] [0, 2] [3, 1] [3, 4]\n",
      "310 [3, 2] [0, 1] [0, 1] [3, 1] [2, 0]\n",
      "311 [2, 1] [0, 1] [1, 0] [0, 2] [2, 1]\n",
      "312 [0, 2] [1, 4] [0, 4] [1, 0] [3, 0]\n",
      "313 [0, 2] [1, 3] [1, 3] [0, 1] [2, 1]\n",
      "314 [2, 1] [0, 1] [1, 2] [0, 2] [2, 0]\n",
      "315 [2, 0] [0, 1] [1, 0] [2, 0] [0, 1]\n",
      "316 [6, 4] [7, 5] [1, 7] [2, 3] [0, 2]\n",
      "317 [3, 2] [0, 1] [0, 1] [3, 1] [1, 2]\n",
      "318 [3, 2] [1, 2] [2, 0] [0, 1] [2, 3]\n",
      "319 [1, 2] [0, 1] [0, 1] [0, 1] [0, 1]\n",
      "320 [3, 0] [2, 1] [0, 2] [2, 3] [3, 1]\n",
      "321 [5, 0] [2, 6] [0, 1] [4, 1] [5, 4]\n",
      "322 [2, 0] [3, 0] [0, 1] [0, 2] [1, 2]\n",
      "323 [0, 3] [4, 2] [4, 0] [1, 3] [1, 4]\n",
      "324 [2, 0] [1, 0] [2, 0] [0, 2] [1, 0]\n",
      "325 [0, 2] [1, 2] [0, 1] [2, 1] [1, 2]\n",
      "326 [3, 2] [0, 4] [2, 3] [2, 1] [2, 0]\n",
      "327 [0, 1] [2, 1] [2, 0] [0, 2] [2, 0]\n",
      "328 [2, 7] [3, 5] [4, 1] [0, 6] [6, 0]\n",
      "329 [2, 1] [0, 1] [0, 1] [1, 2] [2, 0]\n",
      "330 [1, 2] [2, 1] [2, 0] [0, 2] [0, 1]\n",
      "331 [1, 2] [0, 2] [0, 2] [1, 0] [2, 1]\n",
      "332 [1, 2] [3, 0] [0, 1] [1, 3] [0, 1]\n",
      "333 [4, 0] [3, 0] [1, 2] [2, 1] [1, 0]\n",
      "334 [0, 4] [3, 1] [0, 1] [0, 1] [3, 2]\n",
      "335 [0, 1] [1, 0] [0, 1] [0, 1] [0, 1]\n",
      "336 [0, 4] [1, 3] [3, 0] [0, 3] [1, 2]\n",
      "337 [1, 3] [4, 2] [3, 0] [3, 5] [2, 5]\n",
      "338 [0, 4] [3, 1] [0, 1] [4, 1] [2, 3]\n",
      "339 [0, 1] [1, 0] [1, 0] [1, 0] [0, 1]\n",
      "340 [4, 3] [2, 1] [4, 0] [2, 3] [1, 2]\n",
      "341 [2, 4] [0, 3] [2, 3] [4, 1] [3, 2]\n",
      "342 [1, 2] [3, 0] [0, 2] [1, 3] [0, 3]\n",
      "343 [1, 4] [3, 0] [1, 3] [4, 3] [1, 0]\n",
      "344 [0, 2] [2, 4] [2, 3] [2, 5] [4, 3]\n",
      "345 [2, 4] [1, 3] [4, 0] [3, 0] [3, 1]\n",
      "346 [1, 0] [2, 3] [1, 2] [3, 0] [4, 2]\n",
      "347 [0, 1] [2, 3] [1, 2] [2, 0] [0, 3]\n",
      "348 [3, 2] [0, 2] [0, 1] [1, 4] [1, 4]\n",
      "349 [0, 2] [2, 0] [0, 1] [1, 0] [1, 0]\n",
      "350 [0, 1] [0, 1] [0, 1] [0, 1] [0, 1]\n",
      "351 [0, 1] [2, 3] [3, 0] [3, 0] [3, 0]\n",
      "352 [2, 0] [1, 0] [2, 0] [0, 1] [1, 2]\n",
      "353 [1, 4] [3, 4] [0, 2] [2, 4] [0, 4]\n",
      "354 [1, 0] [2, 3] [0, 2] [2, 3] [2, 0]\n",
      "355 [4, 1] [2, 1] [2, 3] [1, 3] [3, 1]\n",
      "356 [0, 1] [1, 0] [0, 1] [1, 0] [1, 0]\n",
      "357 [0, 1] [2, 1] [2, 0] [1, 0] [2, 0]\n",
      "358 [0, 1] [1, 0] [1, 0] [0, 1] [0, 1]\n",
      "359 [0, 1] [3, 4] [1, 5] [0, 2] [5, 0]\n",
      "360 [1, 2] [3, 0] [0, 1] [4, 1] [0, 4]\n",
      "361 [2, 1] [0, 1] [1, 0] [2, 0] [0, 2]\n",
      "362 [3, 0] [2, 1] [2, 0] [0, 1] [2, 0]\n",
      "363 [3, 0] [2, 1] [2, 0] [0, 1] [2, 1]\n",
      "364 [4, 1] [1, 0] [0, 3] [4, 0] [2, 3]\n",
      "365 [1, 3] [0, 4] [0, 1] [2, 0] [0, 3]\n",
      "366 [2, 1] [0, 2] [1, 0] [1, 2] [1, 2]\n",
      "367 [1, 0] [2, 3] [0, 2] [1, 2] [2, 0]\n",
      "368 [2, 0] [1, 3] [3, 0] [2, 1] [2, 0]\n",
      "369 [1, 0] [0, 1] [0, 1] [1, 0] [0, 1]\n",
      "370 [4, 1] [2, 3] [0, 2] [1, 2] [2, 3]\n",
      "371 [1, 3] [4, 5] [2, 0] [3, 0] [3, 6]\n",
      "372 [3, 0] [0, 2] [0, 2] [1, 0] [3, 2]\n",
      "373 [1, 0] [2, 0] [2, 0] [2, 0] [1, 2]\n",
      "374 [2, 1] [4, 3] [2, 3] [2, 0] [4, 2]\n",
      "375 [4, 6] [1, 2] [1, 2] [4, 6] [2, 5]\n",
      "376 [0, 5] [4, 1] [0, 6] [5, 6] [0, 4]\n",
      "377 [0, 5] [2, 1] [3, 0] [1, 5] [1, 2]\n",
      "378 [1, 4] [0, 3] [2, 1] [2, 1] [0, 3]\n",
      "379 [3, 1] [0, 1] [1, 3] [3, 1] [0, 3]\n",
      "380 [1, 3] [2, 4] [0, 1] [3, 0] [5, 3]\n",
      "381 [6, 3] [8, 7] [0, 2] [6, 5] [0, 3]\n",
      "382 [0, 1] [0, 1] [0, 1] [2, 1] [1, 2]\n",
      "383 [0, 1] [2, 1] [2, 0] [2, 0] [0, 2]\n",
      "384 [0, 2] [3, 1] [3, 0] [2, 3] [2, 0]\n",
      "385 [3, 4] [1, 5] [0, 1] [0, 4] [5, 2]\n",
      "386 [1, 2] [3, 2] [0, 1] [0, 1] [2, 1]\n",
      "387 [2, 6] [1, 5] [1, 2] [3, 6] [4, 0]\n",
      "388 [1, 0] [3, 1] [0, 3] [0, 3] [3, 0]\n",
      "389 [3, 4] [5, 2] [0, 1] [4, 0] [0, 4]\n",
      "390 [1, 3] [2, 0] [3, 0] [3, 1] [2, 0]\n",
      "391 [1, 0] [2, 0] [0, 1] [1, 2] [0, 1]\n",
      "392 [2, 0] [1, 3] [2, 3] [2, 1] [1, 0]\n",
      "393 [5, 2] [0, 3] [1, 2] [5, 1] [5, 4]\n",
      "394 [2, 1] [0, 1] [1, 2] [1, 0] [0, 2]\n",
      "395 [0, 3] [2, 3] [3, 0] [1, 2] [1, 0]\n",
      "396 [3, 1] [0, 3] [0, 1] [2, 1] [2, 3]\n",
      "397 [4, 3] [0, 1] [1, 0] [4, 0] [3, 0]\n",
      "398 [3, 0] [1, 4] [3, 4] [0, 2] [1, 4]\n",
      "399 [2, 3] [1, 0] [2, 1] [4, 2] [1, 0]\n",
      "400 [0, 1] [1, 0] [1, 0] [0, 1] [1, 0]\n",
      "401 [1, 0] [4, 3] [4, 0] [1, 4] [1, 4]\n",
      "402 [3, 2] [1, 0] [0, 2] [1, 3] [2, 0]\n",
      "403 [3, 1] [3, 1] [3, 0] [2, 1] [1, 2]\n",
      "404 [0, 1] [3, 1] [2, 0] [0, 1] [0, 3]\n",
      "405 [3, 4] [5, 3] [0, 1] [1, 0] [1, 5]\n",
      "406 [1, 0] [2, 0] [0, 1] [2, 0] [1, 0]\n",
      "407 [1, 2] [0, 2] [0, 1] [1, 0] [2, 1]\n",
      "408 [1, 0] [4, 3] [1, 2] [2, 3] [4, 1]\n",
      "409 [1, 0] [2, 3] [2, 0] [0, 2] [0, 2]\n",
      "410 [1, 3] [0, 4] [0, 2] [3, 0] [3, 4]\n",
      "411 [0, 4] [3, 5] [0, 1] [1, 0] [3, 1]\n",
      "412 [1, 0] [0, 1] [0, 1] [1, 0] [0, 1]\n",
      "413 [3, 4] [6, 7] [7, 0] [5, 1] [2, 1]\n",
      "414 [3, 0] [4, 5] [4, 0] [2, 0] [4, 1]\n",
      "415 [1, 0] [3, 2] [3, 2] [1, 3] [0, 1]\n",
      "416 [1, 0] [2, 0] [0, 2] [2, 1] [0, 1]\n",
      "417 [2, 4] [3, 4] [0, 3] [0, 2] [3, 2]\n",
      "418 [1, 4] [0, 3] [1, 0] [3, 1] [4, 3]\n",
      "419 [2, 1] [0, 3] [3, 0] [2, 0] [0, 3]\n",
      "420 [0, 2] [3, 1] [2, 3] [3, 0] [2, 1]\n",
      "421 [0, 1] [4, 2] [0, 1] [2, 3] [0, 1]\n",
      "422 [3, 0] [0, 1] [0, 1] [3, 0] [3, 0]\n",
      "423 [1, 0] [2, 1] [1, 0] [1, 2] [2, 1]\n",
      "424 [1, 0] [2, 3] [2, 0] [1, 2] [2, 0]\n",
      "425 [2, 0] [1, 0] [0, 1] [1, 0] [2, 0]\n",
      "426 [1, 2] [3, 0] [0, 1] [1, 0] [1, 0]\n",
      "427 [5, 6] [2, 3] [0, 1] [4, 6] [3, 1]\n",
      "428 [1, 0] [3, 2] [0, 1] [1, 3] [0, 3]\n",
      "429 [0, 1] [5, 2] [3, 0] [0, 2] [1, 3]\n",
      "430 [7, 2] [0, 8] [3, 5] [7, 3] [3, 2]\n",
      "431 [2, 3] [0, 1] [0, 3] [1, 0] [2, 3]\n",
      "432 [1, 2] [2, 0] [2, 1] [1, 2] [0, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 [0, 2] [2, 1] [2, 0] [1, 2] [1, 2]\n",
      "434 [5, 1] [2, 3] [2, 0] [0, 5] [0, 6]\n",
      "435 [2, 1] [1, 0] [0, 3] [2, 1] [4, 0]\n",
      "436 [1, 0] [0, 1] [1, 0] [0, 1] [1, 0]\n",
      "437 [0, 4] [2, 4] [0, 1] [1, 3] [2, 0]\n",
      "438 [3, 1] [1, 2] [1, 0] [1, 0] [2, 1]\n",
      "439 [4, 1] [3, 5] [0, 3] [1, 0] [0, 2]\n",
      "440 [3, 7] [0, 2] [4, 0] [1, 7] [5, 1]\n",
      "441 [2, 3] [1, 0] [0, 1] [2, 1] [3, 1]\n"
     ]
    }
   ],
   "source": [
    "count_same_results = 0\n",
    "\n",
    "for i in range(len(preprocessed_data)):\n",
    "    # LSA\n",
    "    lsa_tfidf_result = lsa_tfidf.summarize(preprocessed_data[i])\n",
    "    lsa_binary_result = lsa_binary.summarize(preprocessed_data[i])\n",
    "    \n",
    "    # MWPE\n",
    "    multiword_result = mwpe.summarize(preprocessed_data[i])\n",
    "    \n",
    "    # TextRank\n",
    "    tr_jaccard_result = tr_jaccard.summarize(preprocessed_data[i])\n",
    "    tr_cosine_result = tr_cosine.summarize(preprocessed_data[i])\n",
    "    \n",
    "    if len(preprocessed_data[i]) > 1:\n",
    "        print(i, lsa_tfidf_result, lsa_binary_result, multiword_result, tr_jaccard_result, tr_cosine_result)\n",
    "    else:\n",
    "        print(i, 'passage only has 1 sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jews originated as a national and religious group in the middle east during the second millennium bce in the part of the levant known as the land of israel\n",
      "1 the merneptah stele appears to confirm the existence of a people of israel associated with the god el somewhere in canaan as far back as the 13th century bce\n",
      "2 the israelites as an outgrowth of the canaanite population consolidated their hold with the emergence of the kingdom of israel and the kingdom of judah\n",
      "3 some consider that these canaanite sedentary israelites melded with incoming nomadic groups known as hebrews\n",
      "4 though few sources in the bible mention the exilic periods in detail the experience of diaspora life from the ancient egyptian rule over the levant to assyrian captivity and exile to babylonian captivity and exile to seleucid imperial rule to the roman occupation and the historical relations between israelites and the homeland became a major feature of jewish history identity and memory\n"
     ]
    }
   ],
   "source": [
    "idx = 211\n",
    "\n",
    "for i in range(len(preprocessed_data[idx])):\n",
    "    print(i, ' '.join(preprocessed_data[idx][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 11\n",
      "120 10\n",
      "130 10\n",
      "135 10\n",
      "143 11\n",
      "173 11\n",
      "201 12\n",
      "203 11\n",
      "222 11\n",
      "297 10\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "original_test_data = []\n",
    "indices = []\n",
    "for i in range(len(preprocessed_data)):\n",
    "    if len(preprocessed_data[i]) >= 10:\n",
    "        test_data.append(preprocessed_data[i])\n",
    "        original_test_data.append(data[i])\n",
    "        indices.append(i)\n",
    "        print(i, len(preprocessed_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'passage': original_test_data, 'index': indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In August 1836, two real estate entrepreneurs—...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 1636 George, Duke of Brunswick-Lüneburg, ru...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Known during development as Xbox Next, Xenon, ...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biodiversity, a contraction of \"biological div...</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John was born to Henry II of England and Elean...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             passage  index\n",
       "0  In August 1836, two real estate entrepreneurs—...    107\n",
       "1  In 1636 George, Duke of Brunswick-Lüneburg, ru...    120\n",
       "2  Known during development as Xbox Next, Xenon, ...    130\n",
       "3  Biodiversity, a contraction of \"biological div...    135\n",
       "4  John was born to Henry II of England and Elean...    143"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('dataset/test_sentence_selection_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare labeled data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv('dataset/test_sentence_selection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>passage</th>\n",
       "      <th>index</th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>In August 1836, two real estate entrepreneurs—...</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In 1636 George, Duke of Brunswick-Lüneburg, ru...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Known during development as Xbox Next, Xenon, ...</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Biodiversity, a contraction of \"biological div...</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>John was born to Henry II of England and Elean...</td>\n",
       "      <td>143</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            passage  index  \\\n",
       "0           0  In August 1836, two real estate entrepreneurs—...    107   \n",
       "1           1  In 1636 George, Duke of Brunswick-Lüneburg, ru...    120   \n",
       "2           2  Known during development as Xbox Next, Xenon, ...    130   \n",
       "3           3  Biodiversity, a contraction of \"biological div...    135   \n",
       "4           4  John was born to Henry II of England and Elean...    143   \n",
       "\n",
       "   first  second  third  \n",
       "0      0       7      1  \n",
       "1      0       2      3  \n",
       "2      0       8      7  \n",
       "3      0       9      3  \n",
       "4      6       8      9  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = labeled_data['first'].tolist()\n",
    "second = labeled_data['second'].tolist()\n",
    "third = labeled_data['third'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_summary = []\n",
    "for i in range(len(first)):\n",
    "    idx = labeled_data['index'].tolist()[i]\n",
    "    gold = ' '.join(preprocessed_data[idx][first[i]]) + '. ' + ' '.join(preprocessed_data[idx][second[i]]) + '. ' + ' '.join(preprocessed_data[idx][third[i]]) + '.'\n",
    "    gold_summary.append(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare results from methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(array, preprocessed_data):\n",
    "    summary = ' '.join(preprocessed_data[array[0]]) + '. ' + ' '.join(preprocessed_data[array[1]]) + '. ' + ' '.join(preprocessed_data[array[2]]) + '.'\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_jaccard = TextRank(similarity='jaccard')\n",
    "tr_cosine = TextRank(similarity='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiWordPhraseExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwpe = MultiWordPhraseExtractor(window_size=5, top_keywords=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_tfidf = SteinbergerJezekLSA(matrix_technique='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_binary = SteinbergerJezekLSA(matrix_technique='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_tfidf_results = []\n",
    "lsa_binary_results = []\n",
    "multiword_results = []\n",
    "tr_jaccard_results = []\n",
    "tr_cosine_results = []\n",
    "\n",
    "for i in labeled_data['index'].tolist():\n",
    "    # LSA\n",
    "    lsa_tfidf_results.append(get_summary(lsa_tfidf.summarize(preprocessed_data[i], top=3), preprocessed_data[i]))\n",
    "    lsa_binary_results.append(get_summary(lsa_binary.summarize(preprocessed_data[i], top=3), preprocessed_data[i]))\n",
    "    \n",
    "    # MWPE\n",
    "    multiword_results.append(get_summary(mwpe.summarize(preprocessed_data[i], top=3), preprocessed_data[i]))\n",
    "    \n",
    "    # TextRank\n",
    "    tr_jaccard_results.append(get_summary(tr_jaccard.summarize(preprocessed_data[i], top=3), preprocessed_data[i]))\n",
    "    tr_cosine_results.append(get_summary(tr_cosine.summarize(preprocessed_data[i], top=3), preprocessed_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    lsa_tfidf_results,\n",
    "    lsa_binary_results,\n",
    "    multiword_results,\n",
    "    tr_jaccard_results,\n",
    "    tr_cosine_results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = [3, 4, 5]\n",
    "keywords = [5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5\n",
      "['sizable numbers', 'sizable older', 'allenfrom john']\n",
      "['iv semi-salic', 'iv law', 'iv william', 'succession law']\n",
      "['mac g5', 'mac power', 'mac apple', 'mac hardware']\n",
      "['pacific sea', 'pacific surface', 'pacific western']\n",
      "['histories governance', 'histories own', 'histories traditions']\n",
      "['bank peninsula', 'bank west', 'bank sinai', 'peninsula part']\n",
      "['mercury freddie', 'mercury smile', 'mercury band']\n",
      "['gdr period', 'gdr church', 'gdr membership', 'gdr lutheranism']\n",
      "['agrarianism mohism', 'agrarianism confucianism', 'agrarianism legalism', 'taoism agrarianism']\n",
      "['direct state', 'direct free', 'irish state', 'mention direct']\n",
      "3 6\n",
      "['sizable numbers', 'sizable older', 'allenfrom john', 'allenfrom new']\n",
      "['iv semi-salic', 'iv law', 'iv william', 'succession law', 'male succession']\n",
      "['mac hardware', 'mac g5', 'mac power', 'mac apple', 'power spotted']\n",
      "['pacific western', 'pacific sea', 'pacific surface', 'temperature sea']\n",
      "['histories governance', 'histories own', 'histories traditions']\n",
      "['bank peninsula', 'bank west', 'bank sinai', 'peninsula part', 'lebanon part']\n",
      "['mercury freddie', 'mercury smile', 'mercury band', 'heart sheer']\n",
      "['gdr period', 'gdr church', 'gdr membership', 'gdr lutheranism', 'church protestant']\n",
      "['agrarianism mohism', 'agrarianism confucianism', 'agrarianism legalism', 'taoism agrarianism', 'philosophies confucianism']\n",
      "['state free', 'direct state', 'irish state', 'mention state', 'irish parliament']\n",
      "3 7\n",
      "['sizable numbers', 'sizable older', 'allenfrom john', 'allenfrom new', 'allenfrom york']\n",
      "['iv semi-salic', 'iv law', 'iv william', 'succession law', 'male succession', 'male possible']\n",
      "['mac hardware', 'mac g5', 'mac power', 'mac apple', 'power spotted', 'kits spotted']\n",
      "['pacific sea', 'sea surface', 'western sea', 'temperature sea', 'climate high']\n",
      "['histories governance', 'histories own', 'histories traditions', 'normandy seaboardanjou']\n",
      "['bank peninsula', 'bank west', 'bank sinai', 'peninsula part', 'lebanon part', 'southern part']\n",
      "['mercury freddie', 'mercury smile', 'mercury band', 'heart sheer', 'attack sheer']\n",
      "['gdr period', 'gdr church', 'gdr membership', 'gdr lutheranism', 'church protestant', 'lutheranism thuringia']\n",
      "['taoism agrarianism', 'taoism mohism', 'agrarianism confucianism', 'agrarianism legalism', 'philosophies confucianism', 'philosophies major']\n",
      "['mention direct', 'irish state', 'irish free', 'irish parliament', 'monarch mention']\n",
      "4 5\n",
      "['allenfrom john', 'allenfrom new', 'allenfrom york', 'allenfrom kirby']\n",
      "['iv semi-salic', 'iv law', 'iv william']\n",
      "['mac hardware', 'mac g5', 'mac power', 'mac apple']\n",
      "['pacific temperature', 'pacific western', 'pacific sea', 'pacific surface']\n",
      "['histories structures', 'histories governance', 'histories own', 'histories traditions']\n",
      "['executive head', 'executive ben-gurion', 'executive david', 'executive zionist']\n",
      "['heart attack', 'heart release', 'heart sheer', 'year heart']\n",
      "['prominent thuringia', 'prominent christian', 'prominent protestant', 'denomination prominent']\n",
      "['taoism agrarianism', 'taoism mohism', 'agrarianism confucianism', 'taoism legalism']\n",
      "['irish direct', 'irish state', 'irish free']\n",
      "4 6\n",
      "['john new', 'john kirby', 'john york', 'allenfrom john', 'new slave']\n",
      "['iv semi-salic', 'iv law', 'iv william', 'male succession']\n",
      "['mac power', 'mac apple', 'mac spotted', 'mac hardware', 'mac g5']\n",
      "['pacific coasts', 'pacific temperature', 'pacific western', 'pacific sea']\n",
      "['histories structures', 'histories governance', 'histories own', 'histories traditions', 'parts own']\n",
      "['executive zionist', 'executive ben-gurion', 'executive day', 'zionist organization', 'executive head']\n",
      "['year heart', 'year attack', 'heart release', 'heart sheer', 'year night']\n",
      "['gdr lutheranism', 'denomination prominent', 'gdr period', 'prominent reformation']\n",
      "['taoism agrarianism', 'taoism mohism', 'agrarianism confucianism', 'taoism legalism', 'philosophies confucianism']\n",
      "['irish direct', 'irish state', 'irish free', 'monarch mention']\n",
      "4 7\n",
      "['john new', 'john york', 'john kirby', 'allenfrom john', 'new slave', 'john chapman']\n",
      "['iv semi-salic', 'iv law', 'iv william', 'line succession', 'male succession']\n",
      "['mac hardware', 'mac g5', 'kits alpha', 'mac power', 'kits apple']\n",
      "['pacific coasts', 'pacific temperature', 'pacific western', 'pacific sea', 'climate high']\n",
      "['histories structures', 'histories governance', 'histories own', 'histories traditions', 'parts own']\n",
      "['executive zionist', 'executive ben-gurion', 'executive day', 'zionist organization', 'executive head', 'executive david']\n",
      "['year heart', 'year attack', 'heart release', 'heart sheer', 'year night', 'opera night']\n",
      "['gdr lutheranism', 'denomination prominent', 'gdr period', 'prominent reformation', 'denomination christian']\n",
      "['taoism agrarianism', 'taoism mohism', 'agrarianism confucianism', 'taoism legalism', 'philosophies confucianism']\n",
      "['irish direct', 'irish state', 'irish free', 'monarch mention', 'irish parliament']\n",
      "5 5\n",
      "['estate entrepreneursaugustus', 'estate john']\n",
      "['rank holy', 'rank emperor', 'rank roman', 'holy elevated']\n",
      "['mac g5', 'mac power', 'mac apple', 'mac spotted']\n",
      "['pacific temperature', 'pacific western', 'pacific sea', 'pacific surface']\n",
      "['normandy seaboardanjou', 'normandy territories', 'normandy atlantic']\n",
      "['mandatory plan', 'mandatory un', 'jewish borders']\n",
      "['rhapsody bohemian', 'rhapsody success', 'rhapsody latter']\n",
      "['denomination lutheranism', 'denomination prominent', 'denomination reformation', 'denomination christian']\n",
      "['taoism agrarianism', 'taoism mohism']\n",
      "['irish parliament', 'irish mention', 'irish direct', 'irish state']\n",
      "5 6\n",
      "['estate entrepreneursaugustus', 'estate john', 'estate chapman', 'allenfrom new']\n",
      "['rank holy', 'rank prince-elector', 'rank emperor', 'holy elevated', 'rank roman']\n",
      "['mac g5', 'mac power', 'mac apple', 'mac spotted', 'kits power']\n",
      "['pacific temperature', 'pacific western', 'pacific sea', 'pacific surface', 'pacific coasts']\n",
      "['histories structures', 'empire normandy', 'histories governance', 'histories parts']\n",
      "['mandatory plan', 'mandatory un', 'jewish borders', 'mandatory partition']\n",
      "['rhapsody number', 'rhapsody bohemian', 'rhapsody success', 'rhapsody latter']\n",
      "['denomination lutheranism', 'denomination prominent', 'denomination reformation', 'denomination christian', 'denomination thuringia']\n",
      "['taoism agrarianism', 'taoism mohism', 'philosophy confucianism']\n",
      "['irish state', 'irish free', 'irish mention', 'irish direct', 'irish parliament']\n",
      "5 7\n",
      "['estate entrepreneursaugustus', 'estate john', 'estate chapman', 'allenfrom new']\n",
      "['rank holy', 'rank prince-elector', 'rank emperor', 'holy elevated', 'rank roman', 'dukes elevated']\n",
      "['mac g5', 'mac power', 'mac apple', 'mac spotted', 'kits power', 'mac hardware']\n",
      "['pacific temperature', 'pacific western', 'pacific sea', 'pacific surface', 'pacific coasts', 'highest western']\n",
      "['normandy seaboardanjou', 'normandy atlantic', 'normandy territories', 'histories own', 'normandy englandand']\n",
      "['mandatory plan', 'mandatory un', 'jewish borders', 'mandatory partition', 'jewish arab']\n",
      "['rhapsody number', 'rhapsody bohemian', 'rhapsody success', 'rhapsody latter']\n",
      "['denomination prominent', 'denomination christian', 'denomination thuringia', 'prominent protestant', 'denomination reformation', 'denomination lutheranism']\n",
      "['taoism agrarianism', 'taoism mohism', 'philosophy confucianism']\n",
      "['irish parliament', 'irish mention', 'irish direct', 'irish state']\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for w in window_size:\n",
    "    for k in keywords:\n",
    "        print(w, k)\n",
    "        mwpe = MultiWordPhraseExtractor(window_size=w, top_keywords=k)\n",
    "        multiword_results = []\n",
    "        for i in labeled_data['index'].tolist():\n",
    "            # MWPE\n",
    "            multiword_results.append(get_summary(mwpe.summarize(preprocessed_data[i], top=3), preprocessed_data[i]))\n",
    "            print(mwpe.phrases)\n",
    "        results.append(multiword_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-based measures (Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(text1, text2):\n",
    "    vec1 = text_to_vector(text1)\n",
    "    vec2 = text_to_vector(text2)\n",
    "    \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = []\n",
    "for method_id in range(len(results)):\n",
    "    cosine_row = []\n",
    "    for test_id in range(len(gold_summary)):\n",
    "        cosine_row.append(get_cosine(gold_summary[test_id], results[method_id][test_id]))\n",
    "    cosine_similarity.append(cosine_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df = pd.DataFrame({\n",
    "    'index': indices,\n",
    "    'lsa_tfidf': cosine_similarity[0],\n",
    "    'lsa_binary': cosine_similarity[1],\n",
    "    'mwpe': cosine_similarity[2],\n",
    "    'tr_jaccard': cosine_similarity[3],\n",
    "    'tr_cosine': cosine_similarity[4],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index         173.100\n",
       "lsa_tfidf       0.481\n",
       "lsa_binary      0.719\n",
       "mwpe            0.783\n",
       "tr_jaccard      0.694\n",
       "tr_cosine       0.669\n",
       "dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result_df.mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lsa_tfidf</th>\n",
       "      <th>lsa_binary</th>\n",
       "      <th>mwpe</th>\n",
       "      <th>tr_jaccard</th>\n",
       "      <th>tr_cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  lsa_tfidf  lsa_binary   mwpe  tr_jaccard  tr_cosine\n",
       "0    107      0.487       0.896  0.931       0.677      0.737\n",
       "1    120      0.527       0.749  0.872       0.932      0.707\n",
       "2    130      0.338       0.715  0.671       0.612      0.620\n",
       "3    135      0.269       0.600  0.613       0.609      0.685\n",
       "4    143      0.542       0.677  0.608       0.579      0.639"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded = eval_result_df.round(3)\n",
    "rounded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded.to_csv('dataset/cosine_without_stopwords_top_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
